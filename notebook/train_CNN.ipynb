{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880ac4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import shutil\n",
    "import PIL\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn import model_selection\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c3aa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '../'\n",
    "SAVED_MODEL_DIR = BASE_DIR + 'model'\n",
    "DATASET_DIR = BASE_DIR + 'dataset/'\n",
    "DATA_DIR = Path(DATASET_DIR)\n",
    "IMG_HEIGHT = 256\n",
    "IMG_WIDTH = 256\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386e8586-334a-43d6-a4e2-72598e566619",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    DATA_DIR,\n",
    "    image_size = (IMG_HEIGHT, IMG_WIDTH),\n",
    "    seed = 123)\n",
    "\n",
    "ds_size = sum(1 for _ in ds.unbatch().take(-1))\n",
    "print(f'[DEBUG] - ds_size: {ds_size}')\n",
    "class_names = ds.class_names\n",
    "ds = ds.shuffle(10000, seed=12)\n",
    "print(f'[INFO] - Class names: {class_names}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fd0568-e24a-4a34-af10-375b7412e306",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "for image, label in tqdm(ds.unbatch().take(-1)):\n",
    "    img = image.numpy().astype(\"uint8\")\n",
    "    x.append(img)\n",
    "    label = label.numpy()\n",
    "    y.append(label)\n",
    "\n",
    "x = np.array(x)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab340f6-2980-43d8-ad96-133027d2a4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val_test, y_train, y_val_test = model_selection.train_test_split(x, y, test_size=0.4, random_state=42)\n",
    "x_val, x_test, y_val, y_test = model_selection.train_test_split(x_val_test, y_val_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7246b167-a7bc-4a93-858f-3f3d2650772e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(class_names)\n",
    "\n",
    "model = Sequential([\n",
    "  layers.experimental.preprocessing.Rescaling(1./255, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
    "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(), \n",
    "  layers.Dropout(0.2),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(64, activation='relu'),\n",
    "  layers.Dense(32, activation='relu'),\n",
    "  layers.Dense(num_classes)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ac86a2-075f-4dd6-b5b5-09162db36ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "epochs = EPOCHS\n",
    "history = model.fit(\n",
    "  x_train, y_train,\n",
    "  validation_data = (x_val, y_val),\n",
    "  epochs = epochs\n",
    ")\n",
    "model.save(SAVED_MODEL_DIR)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e38beb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "loss = history.history['loss']\n",
    "\n",
    "val_acc = history.history['val_accuracy']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label = 'Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label = 'Validation Accuracy')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label = 'Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label = 'Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993f0cc9-871b-4638-bdd9-c4783596aeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.argmax(model.predict(x_test), axis=-1)\n",
    "predictions = predictions.reshape(1,-1)[0]\n",
    "print(classification_report(y_test, predictions, \n",
    "                            target_names = ['correctly-masked (Class 0)','not-masked (Class 1)', 'uncorrectly-masked (Class 2)']))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": "computer-vision",
   "language": "python",
   "name": "computer-vision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
